# Using log4j2 (just for fun) It requires exclusions in spring-boot-starter because by default
# Spring Boot will pick up the logback dependency :/
logging:
  config: classpath:log4j2.xml


spring:
  main:
    banner-mode: "off"
    # We do not need Tomcat running (this application runs from console)
    web-environment: false
  cloud:
    stream:
    
      # We will have 3 brokers and 1 partition. The partition will be replicated/copied in every broker. Just one broker will be the leader.
      # Zookeeper is in charge of choosing the leader.
      # If you want to consume data in order you need one consumer per partition (consumer could be a process or a thread in some Java process,
      # so one Java process could be implementing multiple consumers by means of multiple threads)
      # Consuming data in order: 1 partition + 1 consumer
      # 
      # See: https://kafka.apache.org/documentation/#intro_consumers
      # However, if you require a total order over records this can be achieved with a topic that has only one partition, though
      # this will mean only one consumer process per consumer group.
      instanceCount: 1
      # This is the binder with index 0 (only one consumer/Java process)
      instanceIndex: 0
      
      bindings:
        output:
          binder: kafka
          
          # Topic
          destination: example.topic
          
          # Using Apache Avro with Schema Registry
          # contentType: application/*+avro
          contentType: application/json

          
          producer:
            # payload will be translated to Product.class (I hope)
            # For example, if I had 3 partitions. Product objects with the same key will go to the same partition.
            # Otherwise if I had 3 partitions and no key, objects will be using different partitions in a random way.
            partitionKeyExpression: payload.name
            
            # We have 3 brokers and 1 partition. This one partition will replicated/copied in every broker.
            partitionCount: 1
            
            # Consumer (input) and producer (output) are Spring Cloud Stream applications :)
            headerMode: embeddedHeaders
            

      
      kafka:
        binder:
          # Kafka cluster with 3 brokers
          brokers: "kafka.global-pre.spain.schibsted.io:9092"
          # Zookeeper
          zkNodes: "zookeeper.global-pre.spain.schibsted.io:2181/global"
          
          # I THINK THIS CONFIGURATION IS WORTHLESS... IT IS NEVER USED...
          # see: https://github.com/spring-cloud/spring-cloud-stream-binder-kafka/commit/8362cc35691ff64ce5ad401f43ca141910255b76
          # JUST IN CASE I AM GOING TO SKIP IT, JUST IN CASE THERE COULD BE SOME CONFLICT WITH autoCommitOnError :/
          offsetUpdateTimeWindow: 3000
          
          # This means the leader will wait for the full set of in-sync replicas to acknowledge the record. This guarantees that
          # the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.
          # This is equivalent to the acks=-1 setting.
          requiredAcks: 1
          
          
          # It applies when using autoCreateTopics and autoAddPartitions.
          # minPartitionCount: 1
          
          # I have 3 brokers and 1 partition. This partition must be replicated/copied in the 3 brokers.
          # This option is only useful when using autoCreateTopics (Kafka will create our topics based on this
          # configuration) Because I do not like creating stuff in run time I will be using autoCreateTopics: false
          # and this option does not apply :)
          # replicationFactor: 3
          
          # Topics will be created by me on the Kafka cluster/servers
          autoCreateTopics: true
          # Partitions will be created by me on the Kafka cluster/servers
          autoAddPartitions: true

        bindings:
          output:
            producer:
              sync: true
              # No batch mode.
              batchTimeout: 0
          
          
      # Apache AVRO schema-registry    
      schemaRegistryClient: "http://localhost:8888/schema-registry/"



# METRICS
## When using micrometer-registry-datadog :)
## It sends directly to datadoghq
management:
  # Health indicator for binders.
  health:
    binders:
      enabled: true
  metrics:
    binders:
      jvm:
        enabled: false
      uptime:
        enabled: false
      processor:
        enabled: false
    export:
      datadog:
        api-key:
        enabled: true
        step: 30s
## When using micrometer-registry-statsd
## It sends to the dogstatsd daemon
#      statsd:
#        enabled: true
#        flavor: datadog
#        host: ${STATSD_HOST:localhost}
#        port: ${STATSD_PORT:8125}
